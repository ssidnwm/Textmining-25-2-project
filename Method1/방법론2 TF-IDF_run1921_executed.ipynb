{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T01:27:46.563489Z",
     "iopub.status.busy": "2025-10-04T01:27:46.563489Z",
     "iopub.status.idle": "2025-10-04T01:27:46.574493Z",
     "shell.execute_reply": "2025-10-04T01:27:46.574493Z"
    },
    "tags": [
     "injected_params"
    ]
   },
   "outputs": [],
   "source": [
    "INPUT_CSV = r\"C:\\Users\\82109\\Desktop\\벼리\\7학기\\텍스트마이닝\\scripts\\텍스트마이닝_초록_검사결과_전체컬럼1921.csv\"\n",
    "INPUT_XLSX = INPUT_CSV\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T01:27:46.577493Z",
     "iopub.status.busy": "2025-10-04T01:27:46.576494Z",
     "iopub.status.idle": "2025-10-04T01:27:46.589424Z",
     "shell.execute_reply": "2025-10-04T01:27:46.589424Z"
    },
    "tags": [
     "injected_params"
    ]
   },
   "outputs": [],
   "source": [
    "OUTDIR = r\"C:\\Users\\82109\\Desktop\\벼리\\7학기\\텍스트마이닝\\방법론2\\TF-IDF1921\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T01:27:46.591425Z",
     "iopub.status.busy": "2025-10-04T01:27:46.591425Z",
     "iopub.status.idle": "2025-10-04T01:27:47.567526Z",
     "shell.execute_reply": "2025-10-04T01:27:47.566526Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T01:27:47.570526Z",
     "iopub.status.busy": "2025-10-04T01:27:47.570526Z",
     "iopub.status.idle": "2025-10-04T01:27:47.582329Z",
     "shell.execute_reply": "2025-10-04T01:27:47.582329Z"
    }
   },
   "outputs": [],
   "source": [
    "# 분석할 '그 파일'의 정확한 경로를 직접 넣으세요\n",
    "INPUT_CSV = r\"C:\\Users\\82109\\Desktop\\벼리\\7학기\\텍스트마이닝\\scripts\\텍스트마이닝_초록_검사결과_전체컬럼1921.csv\"\n",
    "TITLE_COL  = \"제목\"     # 없으면 None\n",
    "ABS_COL = \"abstract\"\n",
    "\n",
    "# ===== TF-IDF 옵션 =====\n",
    "USE_BIGRAMS = True   # (1,2) n-gram 사용할지\n",
    "MIN_DF      = 2      # 너무 희귀한 토큰 제거(문서 수 기준)\n",
    "MAX_DF      = 0.95   # 너무 흔한 토큰 제거(문서 비율 기준)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T01:27:47.584335Z",
     "iopub.status.busy": "2025-10-04T01:27:47.584335Z",
     "iopub.status.idle": "2025-10-04T01:27:47.598221Z",
     "shell.execute_reply": "2025-10-04T01:27:47.598221Z"
    }
   },
   "outputs": [],
   "source": [
    "# 한글/영문/숫자 토큰 추출용 정규식 (길이 >= 2)\n",
    "# 예: \"부상 예방\", \"soccer\", \"2019\" → 토큰\n",
    "_TOKEN_RE = re.compile(r\"[가-힣]{2,}|[A-Za-z]{2,}|[0-9]{2,}\")\n",
    "\n",
    "# 필요시 도메인 불용어(자유롭게 추가/수정)\n",
    "STOPWORDS = {\n",
    "    \"연구\", \"결과\", \"방법\", \"분석\", \"본\", \"최근\", \"활용\", \"제시\", \"문헌\",\n",
    "    \"the\", \"and\", \"for\", \"with\", \"from\", \"using\", \"based\", \"study\", \"results\"\n",
    "}\n",
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "    if not isinstance(s, str): return \"\"\n",
    "    s = s.lower()\n",
    "    # 도메인 규칙: fifa 11+ → fifa11plus 로 묶어주기\n",
    "    s = re.sub(r\"fifa\\s*11\\+?\", \"fifa11plus\", s, flags=re.I)\n",
    "    # URL/특수기호 정리\n",
    "    s = re.sub(r\"https?://\\S+|www\\.\\S+\", \" \", s)\n",
    "    s = re.sub(r\"[\\u200b-\\u200d\\ufeff]\", \" \", s)\n",
    "    s = re.sub(r\"[^\\w\\s가-힣+]\", \" \", s)   # +는 11+ 같은 표기를 위해 남겨둠\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "# “문자(한글/영문)가 최소 1개 포함된” 토큰만 추출\n",
    "_TOKEN_RE = re.compile(r\"(?=[\\w+]*[가-힣A-Za-z])[가-힣A-Za-z0-9+]{2,}\")\n",
    "\n",
    "\n",
    "def regex_tokenize(text: str):\n",
    "    text = normalize_text(text)\n",
    "    toks = _TOKEN_RE.findall(text)\n",
    "    # 숫자만으로 구성된 토큰 제거(이중 안전장치)\n",
    "    toks = [t for t in toks if not t.isdigit()]\n",
    "    # 불용어 제거\n",
    "    toks = [t for t in toks if t not in STOPWORDS]\n",
    "    return toks\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T01:27:47.600225Z",
     "iopub.status.busy": "2025-10-04T01:27:47.600225Z",
     "iopub.status.idle": "2025-10-04T01:27:47.979390Z",
     "shell.execute_reply": "2025-10-04T01:27:47.979390Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF shape: (884, 24524)\n",
      "예시 terms: ['1a' '1kg' '1st' '1st 2nd' '20g' '20m' '24h' '24h post' '2b' '2d' '2d 3d'\n",
      " '2d evaluation' '2d or' '2d video' '2kg' '2nd' '3d' '3d biomechanics'\n",
      " '3d kinematics' '3d knee']\n"
     ]
    }
   ],
   "source": [
    "# 1) 데이터 로드\n",
    "assert os.path.exists(INPUT_CSV), f\"파일이 없습니다: {INPUT_XLSX}\"\n",
    "df = pd.read_csv(INPUT_CSV)\n",
    "\n",
    "assert ABS_COL in df.columns, f\"'{ABS_COL}' 컬럼을 찾을 수 없습니다.\"\n",
    "texts = df[ABS_COL].astype(str).tolist()\n",
    "\n",
    "# 2) TF-IDF 빌드 (정규식 토크나이저 사용)\n",
    "ngram = (1, 2) if USE_BIGRAMS else (1, 1)\n",
    "vectorizer = TfidfVectorizer(\n",
    "    tokenizer=regex_tokenize,   # 커스텀 토크나이저\n",
    "    token_pattern=None,         # <- 커스텀 tokenizer 쓸 때 반드시 None\n",
    "    ngram_range=ngram,\n",
    "    min_df=MIN_DF,\n",
    "    max_df=MAX_DF,\n",
    "    stop_words=list(STOPWORDS)\n",
    ")\n",
    "X = vectorizer.fit_transform(texts)               # (문서수, 용어수) CSR 희소행렬\n",
    "terms = np.array(vectorizer.get_feature_names_out())\n",
    "\n",
    "print(\"TF-IDF shape:\", X.shape)\n",
    "print(\"예시 terms:\", terms[:20])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "상위 키워드 5개씩 뽑기 - 논문당"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T01:27:48.000392Z",
     "iopub.status.busy": "2025-10-04T01:27:48.000392Z",
     "iopub.status.idle": "2025-10-04T01:27:48.056560Z",
     "shell.execute_reply": "2025-10-04T01:27:48.056560Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[save] C:\\Users\\82109\\Desktop\\벼리\\7학기\\텍스트마이닝\\방법론2\\TF-IDF1921\\tfidf_top5_per_doc.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>top5_keywords</th>\n",
       "      <th>kw1</th>\n",
       "      <th>kw2</th>\n",
       "      <th>kw3</th>\n",
       "      <th>kw4</th>\n",
       "      <th>kw5</th>\n",
       "      <th>score1</th>\n",
       "      <th>score2</th>\n",
       "      <th>score3</th>\n",
       "      <th>score4</th>\n",
       "      <th>score5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2019-12-31</td>\n",
       "      <td>2019</td>\n",
       "      <td>u15(0.227), adductor(0.219), forces(0.202), ad...</td>\n",
       "      <td>u15</td>\n",
       "      <td>adductor</td>\n",
       "      <td>forces</td>\n",
       "      <td>adductor muscle</td>\n",
       "      <td>increases</td>\n",
       "      <td>0.227</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.202</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2019-12-30</td>\n",
       "      <td>2019</td>\n",
       "      <td>stretching(0.462), static(0.253), eight weeks(...</td>\n",
       "      <td>stretching</td>\n",
       "      <td>static</td>\n",
       "      <td>eight weeks</td>\n",
       "      <td>jump</td>\n",
       "      <td>test</td>\n",
       "      <td>0.462</td>\n",
       "      <td>0.253</td>\n",
       "      <td>0.181</td>\n",
       "      <td>0.148</td>\n",
       "      <td>0.147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2019-12-21</td>\n",
       "      <td>2019</td>\n",
       "      <td>economy(0.262), running economy(0.262), runnin...</td>\n",
       "      <td>economy</td>\n",
       "      <td>running economy</td>\n",
       "      <td>running</td>\n",
       "      <td>research</td>\n",
       "      <td>more</td>\n",
       "      <td>0.262</td>\n",
       "      <td>0.262</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.161</td>\n",
       "      <td>0.148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2019-12-20</td>\n",
       "      <td>2019</td>\n",
       "      <td>faster(0.254), cod(0.244), performers(0.206), ...</td>\n",
       "      <td>faster</td>\n",
       "      <td>cod</td>\n",
       "      <td>performers</td>\n",
       "      <td>greater</td>\n",
       "      <td>displayed greater</td>\n",
       "      <td>0.254</td>\n",
       "      <td>0.244</td>\n",
       "      <td>0.206</td>\n",
       "      <td>0.173</td>\n",
       "      <td>0.151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2019-12-17</td>\n",
       "      <td>2019</td>\n",
       "      <td>ecological dynamics(0.434), ecological(0.400),...</td>\n",
       "      <td>ecological dynamics</td>\n",
       "      <td>ecological</td>\n",
       "      <td>dynamics</td>\n",
       "      <td>place</td>\n",
       "      <td>theories</td>\n",
       "      <td>0.434</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.329</td>\n",
       "      <td>0.304</td>\n",
       "      <td>0.172</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   title        date  year                                      top5_keywords  \\\n",
       "0      0  2019-12-31  2019  u15(0.227), adductor(0.219), forces(0.202), ad...   \n",
       "1      1  2019-12-30  2019  stretching(0.462), static(0.253), eight weeks(...   \n",
       "2      2  2019-12-21  2019  economy(0.262), running economy(0.262), runnin...   \n",
       "3      3  2019-12-20  2019  faster(0.254), cod(0.244), performers(0.206), ...   \n",
       "4      4  2019-12-17  2019  ecological dynamics(0.434), ecological(0.400),...   \n",
       "\n",
       "                   kw1              kw2          kw3              kw4  \\\n",
       "0                  u15         adductor       forces  adductor muscle   \n",
       "1           stretching           static  eight weeks             jump   \n",
       "2              economy  running economy      running         research   \n",
       "3               faster              cod   performers          greater   \n",
       "4  ecological dynamics       ecological     dynamics            place   \n",
       "\n",
       "                 kw5 score1 score2 score3 score4 score5  \n",
       "0          increases  0.227  0.219  0.202  0.136  0.126  \n",
       "1               test  0.462  0.253  0.181  0.148  0.147  \n",
       "2               more  0.262  0.262  0.206  0.161  0.148  \n",
       "3  displayed greater  0.254  0.244  0.206  0.173  0.151  \n",
       "4           theories  0.434  0.400  0.329  0.304  0.172  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "TOPK_PER_DOC = 5  # 문서당 추출할 키워드 개수\n",
    "\n",
    "def topk_keywords_per_doc(X, terms, k=5):\n",
    "    \"\"\"각 문서의 TF-IDF 상위 k개 (term, score) 리스트 반환\"\"\"\n",
    "    results = []\n",
    "    for i in range(X.shape[0]):\n",
    "        row = X.getrow(i)\n",
    "        if row.nnz == 0:\n",
    "            results.append([])\n",
    "            continue\n",
    "        # 상위 k개 인덱스 (k보다 작으면 가능한 만큼)\n",
    "        take = min(k, row.nnz)\n",
    "        part = np.argpartition(row.data, -take)[-take:]      # 상위 k개 *무정렬*\n",
    "        order = part[np.argsort(row.data[part])[::-1]]       # TF-IDF 내림차순 정렬\n",
    "        term_idx = row.indices[order]\n",
    "        scores = row.data[order]\n",
    "        results.append(list(zip(terms[term_idx], scores)))\n",
    "    return results\n",
    "\n",
    "# 1) 문서별 상위 5 키워드 (term, score) 추출\n",
    "topk = topk_keywords_per_doc(X, terms, k=TOPK_PER_DOC)\n",
    "\n",
    "# 2) 결과 DataFrame 구성: 제목 + 상위키워드(문자열)\n",
    "title = df[TITLE_COL] if (TITLE_COL in df.columns) else pd.Index(range(len(df)))\n",
    "date = df[\"date\"] if \"date\" in df.columns else pd.Series([\"\"] * len(df))\n",
    "year = df[\"year\"] if \"year\" in df.columns else pd.Series([\"\"] * len(df))\n",
    "topk_str = [\", \".join([f\"{t}({s:.3f})\" for t, s in pairs]) for pairs in topk]\n",
    "\n",
    "# (옵션) 개별 컬럼으로도 뽑기: kw1..kw5, score1..score5\n",
    "def pad(lst, n, fill=None):  # 길이가 n보다 짧으면 채워 넣기\n",
    "    return lst + [fill] * (n - len(lst))\n",
    "\n",
    "kw_cols = []\n",
    "sc_cols = []\n",
    "for pairs in topk:\n",
    "    terms_only  = pad([t for t, _ in pairs], TOPK_PER_DOC, \"\")\n",
    "    scores_only = pad([f\"{s:.3f}\" for _, s in pairs], TOPK_PER_DOC, \"\")\n",
    "    kw_cols.append(terms_only)\n",
    "    sc_cols.append(scores_only)\n",
    "\n",
    "kw_df = pd.DataFrame(kw_cols, columns=[f\"kw{i+1}\" for i in range(TOPK_PER_DOC)])\n",
    "sc_df = pd.DataFrame(sc_cols, columns=[f\"score{i+1}\" for i in range(TOPK_PER_DOC)])\n",
    "\n",
    "result_df = pd.DataFrame({\"title\": title, \"date\": date,\"year\": year,\"top5_keywords\": topk_str})\n",
    "result_df = pd.concat([result_df, kw_df, sc_df], axis=1)\n",
    "\n",
    "# 3) 저장\n",
    "out_path = os.path.join(OUTDIR, f\"tfidf_top{TOPK_PER_DOC}_per_doc.csv\")\n",
    "result_df.to_csv(out_path, index=False, encoding=\"utf-8-sig\")\n",
    "print(\"[save]\", out_path)\n",
    "\n",
    "# 4) 샘플 확인 (앞 5개)\n",
    "result_df.head()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제, 이 논문들의 키워드를 가지고 클러스터링을 진행함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T01:27:48.058565Z",
     "iopub.status.busy": "2025-10-04T01:27:48.058565Z",
     "iopub.status.idle": "2025-10-04T01:27:48.626070Z",
     "shell.execute_reply": "2025-10-04T01:27:48.625069Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] pooled keywords: (1685, 5) unique: 1283\n",
      "[save] ./TFIDF2021\\pooled_keywords_top5_long.csv\n",
      "X_kw shape: (1685, 8302)\n",
      "   k  silhouette\n",
      "0  8    0.020447\n",
      "[info] choose k = 8\n",
      "[save] ./TFIDF2021\\2021keyword_clusters_k8.csv\n",
      "[save] ./TFIDF2021\\2021keyword_cluster_summary_k8.csv\n",
      "[save] ./TFIDF2021\\2021keyword_cluster_top_docs_k8.csv\n",
      "\n",
      "=== Cluster 0 ===\n",
      "fatigue, cod, concussion, landing, heading, con, rehabilitation, calibration, calibration simulations, classification\n",
      "\n",
      "=== Cluster 1 ===\n",
      "bone, bout, moments, of mds, rts, students, adolescents, adults, age of, angle of\n",
      "\n",
      "=== Cluster 2 ===\n",
      "activity, cognitive, condition, intervention, practice, practices, prediction, reactive, reactive agility, reactive strength\n",
      "\n",
      "=== Cluster 3 ===\n",
      "energy, test, score, teams, balance, cam morphology, core, force, impact biomechanics, incidence\n",
      "\n",
      "=== Cluster 4 ===\n",
      "asymmetry, groin, pain, inside, las, ankle instability, ankle sprain, ankle sprains, as, asymmetries\n",
      "\n",
      "=== Cluster 5 ===\n",
      "h3, acl, aclr, head, hamstring, load, recovery, gt, speed, ankle\n",
      "\n",
      "=== Cluster 6 ===\n",
      "knee, season, knee joint, knee valgus, decreased knee, increased knee, knee abduction, knee brace, knee flexor, knee flexors\n",
      "\n",
      "=== Cluster 7 ===\n",
      "injury, acl injury, injuries, injury prevention, acl injuries, injured leg, injury incidence, injury screening, of injury, sport injury\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "assert \"year\" in df.columns, \" df에 'year' 컬럼이 있어야 합니다.\"\n",
    "assert \"title\" in df.columns, \" df에 'title' 컬럼이 있어야 합니다.\"\n",
    "\n",
    "YEAR_TARGET = 2021         \n",
    "TITLE_COL   = \"title\"\n",
    "DATE_COL    = \"date\"\n",
    "YEAR_COL    = \"year\"\n",
    "\n",
    "OUTDIR = \"./TFIDF2021\" # 여기 연도별로 바꾸기\n",
    "os.makedirs(OUTDIR, exist_ok=True)\n",
    "\n",
    "mask_year = (df[YEAR_COL] == YEAR_TARGET)\n",
    "df_year = df.loc[mask_year].reset_index(drop=True)\n",
    "if df_year.empty:\n",
    "    raise ValueError(f\"{YEAR_TARGET}년에 해당하는 문서가 없습니다.\")\n",
    "\n",
    "idx_year = np.where(mask_year.values)[0]\n",
    "topk_year = [topk[i] for i in idx_year]\n",
    "# ===============================\n",
    "# 1) 문서별 top-k 결과(topk)를 \"길게\" 풀기\n",
    "#    topk: List[List[(term, score)]]\n",
    "# ===============================\n",
    "rows = []\n",
    "for i, pairs in enumerate(topk_year):\n",
    "    title = df[TITLE_COL].iloc[i] if TITLE_COL in df.columns else i\n",
    "    for term, val in pairs:\n",
    "        rows.append({\n",
    "            \"year\": YEAR_TARGET,\n",
    "            \"doc_id\": i,\n",
    "            \"title\": title,\n",
    "            \"keyword\": term,\n",
    "            \"tfidf\": float(val)\n",
    "        })\n",
    "\n",
    "kw_long = pd.DataFrame(rows)\n",
    "print(\"[info] pooled keywords:\", kw_long.shape, \"unique:\", kw_long[\"keyword\"].nunique())\n",
    "kw_long_path = os.path.join(OUTDIR, \"pooled_keywords_top5_long.csv\")\n",
    "kw_long.to_csv(kw_long_path, index=False, encoding=\"utf-8-sig\")\n",
    "print(\"[save]\", kw_long_path)\n",
    "\n",
    "# ===============================\n",
    "# 2) 키워드 자체를 벡터화\n",
    "#    - 키워드가 짧으니 char n-gram이 안정적\n",
    "#    - 공백이 있는 키워드(예: 'injury prevention')도 잘 커버\n",
    "# ===============================\n",
    "vectorizer_kw = TfidfVectorizer(\n",
    "    analyzer=\"char_wb\",       # 단어 경계 기반 문자 n-gram\n",
    "    ngram_range=(3,5),        # 3~5그램 권장(짧은 토큰에 강함)\n",
    "    min_df=1\n",
    ")\n",
    "X_kw = vectorizer_kw.fit_transform(kw_long[\"keyword\"])\n",
    "print(\"X_kw shape:\", X_kw.shape)\n",
    "\n",
    "# ===============================\n",
    "# 3) KMeans 클러스터링 (k는 필요에 따라 조절)\n",
    "#    - 실루엣으로 후보 k 점검 후 최종 k 선택\n",
    "# ===============================\n",
    "k_candidates = [8]\n",
    "scores = []\n",
    "for k in k_candidates:\n",
    "    if X_kw.shape[0] > k:\n",
    "        km_tmp = KMeans(n_clusters=k, n_init=\"auto\", random_state=42)\n",
    "        labels_tmp = km_tmp.fit_predict(X_kw)\n",
    "        scores.append(silhouette_score(X_kw, labels_tmp))\n",
    "    else:\n",
    "        scores.append(np.nan)\n",
    "\n",
    "sil_df = pd.DataFrame({\"k\": k_candidates, \"silhouette\": scores})\n",
    "print(sil_df)\n",
    "\n",
    "# 최종 k 선택(직접 지정해도 됨)\n",
    "K_CLUSTERS = 8\n",
    "print(\"[info] choose k =\", K_CLUSTERS)\n",
    "\n",
    "km = KMeans(n_clusters=K_CLUSTERS, n_init=\"auto\", random_state=42)\n",
    "kw_long[\"cluster\"] = km.fit_predict(X_kw)\n",
    "\n",
    "clusters_csv = os.path.join(OUTDIR, f\"{YEAR_TARGET}keyword_clusters_k{K_CLUSTERS}.csv\")\n",
    "kw_long.to_csv(clusters_csv, index=False, encoding=\"utf-8-sig\")\n",
    "print(\"[save]\", clusters_csv)\n",
    "\n",
    "# ===============================\n",
    "# 4) 클러스터 요약 (빈도/대표 키워드/대표 문서)\n",
    "# ===============================\n",
    "# (a) 클러스터별 키워드 빈도 상위 샘플\n",
    "top_kw_by_cluster = (\n",
    "    kw_long.groupby([\"cluster\",\"keyword\"])\n",
    "           .size()\n",
    "           .reset_index(name=\"freq\")\n",
    "           .sort_values([\"cluster\",\"freq\"], ascending=[True, False])\n",
    ")\n",
    "\n",
    "summary_rows = []\n",
    "for c in sorted(kw_long[\"cluster\"].unique()):\n",
    "    sample = top_kw_by_cluster[top_kw_by_cluster[\"cluster\"]==c].head(15)[\"keyword\"].tolist()\n",
    "    summary_rows.append({\"cluster\": c, \"sample_keywords\": \", \".join(sample)})\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "summary_csv = os.path.join(OUTDIR, f\"{YEAR_TARGET}keyword_cluster_summary_k{K_CLUSTERS}.csv\")\n",
    "summary_df.to_csv(summary_csv, index=False, encoding=\"utf-8-sig\")\n",
    "print(\"[save]\", summary_csv)\n",
    "\n",
    "# (b) 클러스터별 대표 문서(해당 클러스터에 속한 키워드의 tf-idf 합 기준 상위)\n",
    "doc_rank = (\n",
    "    kw_long.groupby([\"cluster\",\"title\"])[\"tfidf\"]\n",
    "           .sum()\n",
    "           .reset_index()\n",
    "           .sort_values([\"cluster\",\"tfidf\"], ascending=[True, False])\n",
    ")\n",
    "top_docs = doc_rank.groupby(\"cluster\").head(10)\n",
    "top_docs_csv = os.path.join(OUTDIR, f\"{YEAR_TARGET}keyword_cluster_top_docs_k{K_CLUSTERS}.csv\")\n",
    "top_docs.to_csv(top_docs_csv, index=False, encoding=\"utf-8-sig\")\n",
    "print(\"[save]\", top_docs_csv)\n",
    "\n",
    "# ===============================\n",
    "# 5) 화면에 간단 프린트\n",
    "# ===============================\n",
    "for c in sorted(kw_long[\"cluster\"].unique()):\n",
    "    sample_kw = top_kw_by_cluster[top_kw_by_cluster[\"cluster\"]==c].head(10)[\"keyword\"].tolist()\n",
    "    print(f\"\\n=== Cluster {c} ===\")\n",
    "    print(\", \".join(sample_kw))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
