{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T01:20:18.202314Z",
     "iopub.status.busy": "2025-10-04T01:20:18.202314Z",
     "iopub.status.idle": "2025-10-04T01:20:18.213454Z",
     "shell.execute_reply": "2025-10-04T01:20:18.213454Z"
    },
    "tags": [
     "injected_params"
    ]
   },
   "outputs": [],
   "source": [
    "INPUT_CSV = r\"C:\\Users\\82109\\Desktop\\벼리\\7학기\\텍스트마이닝\\scripts\\텍스트마이닝_초록_검사결과_전체컬럼2224.csv\"\n",
    "INPUT_XLSX = INPUT_CSV\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T01:20:18.216459Z",
     "iopub.status.busy": "2025-10-04T01:20:18.216459Z",
     "iopub.status.idle": "2025-10-04T01:20:18.229458Z",
     "shell.execute_reply": "2025-10-04T01:20:18.229458Z"
    },
    "tags": [
     "injected_params"
    ]
   },
   "outputs": [],
   "source": [
    "OUTDIR = r\"C:\\Users\\82109\\Desktop\\벼리\\7학기\\텍스트마이닝\\방법론2\\TF-IDF2224\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T01:20:18.232459Z",
     "iopub.status.busy": "2025-10-04T01:20:18.232459Z",
     "iopub.status.idle": "2025-10-04T01:20:19.176047Z",
     "shell.execute_reply": "2025-10-04T01:20:19.175043Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T01:20:19.178047Z",
     "iopub.status.busy": "2025-10-04T01:20:19.178047Z",
     "iopub.status.idle": "2025-10-04T01:20:19.190298Z",
     "shell.execute_reply": "2025-10-04T01:20:19.190298Z"
    }
   },
   "outputs": [],
   "source": [
    "# 분석할 '그 파일'의 정확한 경로를 직접 넣으세요\n",
    "INPUT_CSV = r\"C:\\Users\\82109\\Desktop\\벼리\\7학기\\텍스트마이닝\\scripts\\텍스트마이닝_초록_검사결과_전체컬럼2224.csv\"\n",
    "TITLE_COL  = \"제목\"     # 없으면 None\n",
    "ABS_COL = \"abstract\"\n",
    "\n",
    "# ===== TF-IDF 옵션 =====\n",
    "USE_BIGRAMS = True   # (1,2) n-gram 사용할지\n",
    "MIN_DF      = 2      # 너무 희귀한 토큰 제거(문서 수 기준)\n",
    "MAX_DF      = 0.95   # 너무 흔한 토큰 제거(문서 비율 기준)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T01:20:19.192299Z",
     "iopub.status.busy": "2025-10-04T01:20:19.192299Z",
     "iopub.status.idle": "2025-10-04T01:20:19.206311Z",
     "shell.execute_reply": "2025-10-04T01:20:19.206311Z"
    }
   },
   "outputs": [],
   "source": [
    "# 한글/영문/숫자 토큰 추출용 정규식 (길이 >= 2)\n",
    "# 예: \"부상 예방\", \"soccer\", \"2019\" → 토큰\n",
    "_TOKEN_RE = re.compile(r\"[가-힣]{2,}|[A-Za-z]{2,}|[0-9]{2,}\")\n",
    "\n",
    "# 필요시 도메인 불용어(자유롭게 추가/수정)\n",
    "STOPWORDS = {\n",
    "    \"Research\", \"Results\", \"Methods\", \"Analysis\", \"This\", \"Recent\", \"Utilisation\", \"Presentation\", \"Literature\",\n",
    "    \"the\", \"and\", \"for\", \"with\", \"from\", \"using\", \"based\", \"study\", \"results\"\n",
    "}\n",
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "    if not isinstance(s, str): return \"\"\n",
    "    s = s.lower()\n",
    "    # 도메인 규칙: fifa 11+ → fifa11plus 로 묶어주기\n",
    "    s = re.sub(r\"fifa\\s*11\\+?\", \"fifa11plus\", s, flags=re.I)\n",
    "    # URL/특수기호 정리\n",
    "    s = re.sub(r\"https?://\\S+|www\\.\\S+\", \" \", s)\n",
    "    s = re.sub(r\"[\\u200b-\\u200d\\ufeff]\", \" \", s)\n",
    "    s = re.sub(r\"[^\\w\\s가-힣+]\", \" \", s)   # +는 11+ 같은 표기를 위해 남겨둠\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "# “문자(한글/영문)가 최소 1개 포함된” 토큰만 추출\n",
    "_TOKEN_RE = re.compile(r\"(?=[\\w+]*[가-힣A-Za-z])[가-힣A-Za-z0-9+]{2,}\")\n",
    "\n",
    "\n",
    "def regex_tokenize(text: str):\n",
    "    text = normalize_text(text)\n",
    "    toks = _TOKEN_RE.findall(text)\n",
    "    # 숫자만으로 구성된 토큰 제거(이중 안전장치)\n",
    "    toks = [t for t in toks if not t.isdigit()]\n",
    "    # 불용어 제거\n",
    "    toks = [t for t in toks if t not in STOPWORDS]\n",
    "    return toks\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T01:20:19.209300Z",
     "iopub.status.busy": "2025-10-04T01:20:19.208300Z",
     "iopub.status.idle": "2025-10-04T01:20:19.603655Z",
     "shell.execute_reply": "2025-10-04T01:20:19.602655Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF shape: (973, 24518)\n",
      "예시 terms: ['10m' '10m sprint' '11th' '12th' '1960s' '1960s to' '19th' '19th century'\n",
      " '1rm' '1st' '20m' '20m sprint' '20th' '2d' '2d scoring' '2d video'\n",
      " '2d videos' '2nd' '2x2' '30m']\n"
     ]
    }
   ],
   "source": [
    "# 1) 데이터 로드\n",
    "assert os.path.exists(INPUT_CSV), f\"파일이 없습니다: {INPUT_XLSX}\"\n",
    "df = pd.read_csv(INPUT_CSV)\n",
    "\n",
    "assert ABS_COL in df.columns, f\"'{ABS_COL}' 컬럼을 찾을 수 없습니다.\"\n",
    "texts = df[ABS_COL].astype(str).tolist()\n",
    "\n",
    "# 2) TF-IDF 빌드 (정규식 토크나이저 사용)\n",
    "ngram = (1, 2) if USE_BIGRAMS else (1, 1)\n",
    "vectorizer = TfidfVectorizer(\n",
    "    tokenizer=regex_tokenize,   # 커스텀 토크나이저\n",
    "    token_pattern=None,         # <- 커스텀 tokenizer 쓸 때 반드시 None\n",
    "    ngram_range=ngram,\n",
    "    min_df=MIN_DF,\n",
    "    max_df=MAX_DF,\n",
    "    stop_words=list(STOPWORDS)\n",
    ")\n",
    "X = vectorizer.fit_transform(texts)               # (문서수, 용어수) CSR 희소행렬\n",
    "terms = np.array(vectorizer.get_feature_names_out())\n",
    "\n",
    "print(\"TF-IDF shape:\", X.shape)\n",
    "print(\"예시 terms:\", terms[:20])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "상위 키워드 5개씩 뽑기 - 논문당"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T01:20:19.613654Z",
     "iopub.status.busy": "2025-10-04T01:20:19.613654Z",
     "iopub.status.idle": "2025-10-04T01:20:19.681992Z",
     "shell.execute_reply": "2025-10-04T01:20:19.681992Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[save] C:\\Users\\82109\\Desktop\\벼리\\7학기\\텍스트마이닝\\방법론2\\TF-IDF2224\\tfidf_top5_per_doc.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>top5_keywords</th>\n",
       "      <th>kw1</th>\n",
       "      <th>kw2</th>\n",
       "      <th>kw3</th>\n",
       "      <th>kw4</th>\n",
       "      <th>kw5</th>\n",
       "      <th>score1</th>\n",
       "      <th>score2</th>\n",
       "      <th>score3</th>\n",
       "      <th>score4</th>\n",
       "      <th>score5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2022-12-30</td>\n",
       "      <td>2022</td>\n",
       "      <td>strength kg(0.232), kg(0.204), length(0.200), ...</td>\n",
       "      <td>strength kg</td>\n",
       "      <td>kg</td>\n",
       "      <td>length</td>\n",
       "      <td>strength</td>\n",
       "      <td>cm</td>\n",
       "      <td>0.232</td>\n",
       "      <td>0.204</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.183</td>\n",
       "      <td>0.162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2022-12-30</td>\n",
       "      <td>2022</td>\n",
       "      <td>skills(0.400), shooting skills(0.319), shootin...</td>\n",
       "      <td>skills</td>\n",
       "      <td>shooting skills</td>\n",
       "      <td>shooting</td>\n",
       "      <td>mental</td>\n",
       "      <td>eye</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.319</td>\n",
       "      <td>0.293</td>\n",
       "      <td>0.275</td>\n",
       "      <td>0.229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2022-12-22</td>\n",
       "      <td>2022</td>\n",
       "      <td>postural(0.339), postural control(0.323), squa...</td>\n",
       "      <td>postural</td>\n",
       "      <td>postural control</td>\n",
       "      <td>square</td>\n",
       "      <td>control</td>\n",
       "      <td>nine</td>\n",
       "      <td>0.339</td>\n",
       "      <td>0.323</td>\n",
       "      <td>0.258</td>\n",
       "      <td>0.231</td>\n",
       "      <td>0.229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2022-12-21</td>\n",
       "      <td>2022</td>\n",
       "      <td>estimation(0.319), pose estimation(0.264), est...</td>\n",
       "      <td>estimation</td>\n",
       "      <td>pose estimation</td>\n",
       "      <td>estimation models</td>\n",
       "      <td>pose</td>\n",
       "      <td>models</td>\n",
       "      <td>0.319</td>\n",
       "      <td>0.264</td>\n",
       "      <td>0.251</td>\n",
       "      <td>0.236</td>\n",
       "      <td>0.205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2022-12-21</td>\n",
       "      <td>2022</td>\n",
       "      <td>concussion(0.417), attitudes(0.335), knowledge...</td>\n",
       "      <td>concussion</td>\n",
       "      <td>attitudes</td>\n",
       "      <td>knowledge</td>\n",
       "      <td>src</td>\n",
       "      <td>cai</td>\n",
       "      <td>0.417</td>\n",
       "      <td>0.335</td>\n",
       "      <td>0.272</td>\n",
       "      <td>0.210</td>\n",
       "      <td>0.155</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   title        date  year                                      top5_keywords  \\\n",
       "0      0  2022-12-30  2022  strength kg(0.232), kg(0.204), length(0.200), ...   \n",
       "1      1  2022-12-30  2022  skills(0.400), shooting skills(0.319), shootin...   \n",
       "2      2  2022-12-22  2022  postural(0.339), postural control(0.323), squa...   \n",
       "3      3  2022-12-21  2022  estimation(0.319), pose estimation(0.264), est...   \n",
       "4      4  2022-12-21  2022  concussion(0.417), attitudes(0.335), knowledge...   \n",
       "\n",
       "           kw1               kw2                kw3       kw4     kw5 score1  \\\n",
       "0  strength kg                kg             length  strength      cm  0.232   \n",
       "1       skills   shooting skills           shooting    mental     eye  0.400   \n",
       "2     postural  postural control             square   control    nine  0.339   \n",
       "3   estimation   pose estimation  estimation models      pose  models  0.319   \n",
       "4   concussion         attitudes          knowledge       src     cai  0.417   \n",
       "\n",
       "  score2 score3 score4 score5  \n",
       "0  0.204  0.200  0.183  0.162  \n",
       "1  0.319  0.293  0.275  0.229  \n",
       "2  0.323  0.258  0.231  0.229  \n",
       "3  0.264  0.251  0.236  0.205  \n",
       "4  0.335  0.272  0.210  0.155  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "TOPK_PER_DOC = 5  # 문서당 추출할 키워드 개수\n",
    "\n",
    "\n",
    "def topk_keywords_per_doc(X, terms, k=5):\n",
    "    \"\"\"각 문서의 TF-IDF 상위 k개 (term, score) 리스트 반환\"\"\"\n",
    "    results = []\n",
    "    for i in range(X.shape[0]):\n",
    "        row = X.getrow(i)\n",
    "        if row.nnz == 0:\n",
    "            results.append([])\n",
    "            continue\n",
    "        # 상위 k개 인덱스 (k보다 작으면 가능한 만큼)\n",
    "        take = min(k, row.nnz)\n",
    "        part = np.argpartition(row.data, -take)[-take:]      # 상위 k개 *무정렬*\n",
    "        order = part[np.argsort(row.data[part])[::-1]]       # TF-IDF 내림차순 정렬\n",
    "        term_idx = row.indices[order]\n",
    "        scores = row.data[order]\n",
    "        results.append(list(zip(terms[term_idx], scores)))\n",
    "    return results\n",
    "\n",
    "# 1) 문서별 상위 5 키워드 (term, score) 추출\n",
    "topk = topk_keywords_per_doc(X, terms, k=TOPK_PER_DOC)\n",
    "\n",
    "# 2) 결과 DataFrame 구성: 제목 + 상위키워드(문자열)\n",
    "title = df[TITLE_COL] if (TITLE_COL in df.columns) else pd.Index(range(len(df)))\n",
    "date = df[\"date\"] if \"date\" in df.columns else pd.Series([\"\"] * len(df))\n",
    "year = df[\"year\"] if \"year\" in df.columns else pd.Series([\"\"] * len(df))\n",
    "topk_str = [\", \".join([f\"{t}({s:.3f})\" for t, s in pairs]) for pairs in topk]\n",
    "\n",
    "# (옵션) 개별 컬럼으로도 뽑기: kw1..kw5, score1..score5\n",
    "def pad(lst, n, fill=None):  # 길이가 n보다 짧으면 채워 넣기\n",
    "    return lst + [fill] * (n - len(lst))\n",
    "\n",
    "kw_cols = []\n",
    "sc_cols = []\n",
    "for pairs in topk:\n",
    "    terms_only  = pad([t for t, _ in pairs], TOPK_PER_DOC, \"\")\n",
    "    scores_only = pad([f\"{s:.3f}\" for _, s in pairs], TOPK_PER_DOC, \"\")\n",
    "    kw_cols.append(terms_only)\n",
    "    sc_cols.append(scores_only)\n",
    "\n",
    "kw_df = pd.DataFrame(kw_cols, columns=[f\"kw{i+1}\" for i in range(TOPK_PER_DOC)])\n",
    "sc_df = pd.DataFrame(sc_cols, columns=[f\"score{i+1}\" for i in range(TOPK_PER_DOC)])\n",
    "\n",
    "result_df = pd.DataFrame({\"title\": title, \"date\": date,\"year\": year,\"top5_keywords\": topk_str})\n",
    "result_df = pd.concat([result_df, kw_df, sc_df], axis=1)\n",
    "\n",
    "# 3) 저장\n",
    "out_path = os.path.join(OUTDIR, f\"tfidf_top{TOPK_PER_DOC}_per_doc.csv\")\n",
    "result_df.to_csv(out_path, index=False, encoding=\"utf-8-sig\")\n",
    "print(\"[save]\", out_path)\n",
    "\n",
    "# 4) 샘플 확인 (앞 5개)\n",
    "result_df.head()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제, 이 논문들의 키워드를 가지고 클러스터링을 진행함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T01:20:19.684996Z",
     "iopub.status.busy": "2025-10-04T01:20:19.684996Z",
     "iopub.status.idle": "2025-10-04T01:20:21.511687Z",
     "shell.execute_reply": "2025-10-04T01:20:21.511687Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] pooled keywords: (1165, 5) unique: 923\n",
      "[save] ./TFIDF2024\\pooled_keywords_top5_long.csv\n",
      "X_kw shape: (1165, 6844)\n",
      "   k  silhouette\n",
      "0  4    0.011797\n",
      "1  5    0.013052\n",
      "2  6    0.013192\n",
      "3  7    0.014039\n",
      "4  8    0.024378\n",
      "[info] choose k = 8\n",
      "[save] ./TFIDF2024\\2024keyword_clusters_k8.csv\n",
      "[save] ./TFIDF2024\\2024keyword_cluster_summary_k8.csv\n",
      "[save] ./TFIDF2024\\2024keyword_cluster_top_docs_k8.csv\n",
      "\n",
      "=== Cluster 0 ===\n",
      "training, specific training, strain, explaining of, gaining, interval training, neuromuscular training, physical training, plyometric training, sprint training\n",
      "\n",
      "=== Cluster 1 ===\n",
      "ankle, wearable, female, muscle, angle, female athletes, muscles, ankle df, ankle flexion, ankle injuries\n",
      "\n",
      "=== Cluster 2 ===\n",
      "ball, tears, handball, ball kicking, foot, footballers, spanish football, angle ball, australian football, ball release\n",
      "\n",
      "=== Cluster 3 ===\n",
      "h3, cod, sprint, fatigue, group, knee, strength, cai, cmj, dominant\n",
      "\n",
      "=== Cluster 4 ===\n",
      "acl, aclr, acl injuries, acl force, acl injury, acl rupture, acl stress, acli, aclr group, athletes aclr\n",
      "\n",
      "=== Cluster 5 ===\n",
      "et al, anterior cruciate, artificial turf, contact, cortical, natural grass, non surgical, physical, psychological, visual\n",
      "\n",
      "=== Cluster 6 ===\n",
      "head, jump, head impact, header, heading, health, heading incidence, headings, jump height, jump performance\n",
      "\n",
      "=== Cluster 7 ===\n",
      "limb, properties, warm, warm up, amateur soccer, breast support, elite soccer, female soccer, in soccer, inter limb\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "assert \"year\" in df.columns, \" df에 'year' 컬럼이 있어야 합니다.\"\n",
    "assert \"title\" in df.columns, \" df에 'title' 컬럼이 있어야 합니다.\"\n",
    "\n",
    "YEAR_TARGET = 2024          \n",
    "TITLE_COL   = \"title\"\n",
    "DATE_COL    = \"date\"\n",
    "YEAR_COL    = \"year\"\n",
    "\n",
    "OUTDIR = \"./TFIDF2024\" # 여기 연도별로 바꾸기\n",
    "os.makedirs(OUTDIR, exist_ok=True)\n",
    "\n",
    "mask_year = (df[YEAR_COL] == YEAR_TARGET)\n",
    "df_year = df.loc[mask_year].reset_index(drop=True)\n",
    "if df_year.empty:\n",
    "    raise ValueError(f\"{YEAR_TARGET}년에 해당하는 문서가 없습니다.\")\n",
    "\n",
    "# topk가 전체 df 순서 기준으로 만들어졌다고 가정 → 해당 연도 인덱스만 추출\n",
    "# topk를 연도 서브셋으로 축소 (핵심!)\n",
    "idx_year = np.where(mask_year.values)[0]\n",
    "topk_year = [topk[i] for i in idx_year]\n",
    "\n",
    "# ===============================\n",
    "# 1) 문서별 top-k 결과(topk)를 \"길게\" 풀기\n",
    "#    topk: List[List[(term, score)]]\n",
    "# ===============================\n",
    "rows = []\n",
    "for i, pairs in enumerate(topk_year):\n",
    "    title = df[TITLE_COL].iloc[i] if TITLE_COL in df.columns else i\n",
    "    for term, val in pairs:\n",
    "        rows.append({\n",
    "            \"year\": YEAR_TARGET,\n",
    "            \"doc_id\": i,\n",
    "            \"title\": title,\n",
    "            \"keyword\": term,\n",
    "            \"tfidf\": float(val)\n",
    "        })\n",
    "\n",
    "kw_long = pd.DataFrame(rows)\n",
    "print(\"[info] pooled keywords:\", kw_long.shape, \"unique:\", kw_long[\"keyword\"].nunique())\n",
    "kw_long_path = os.path.join(OUTDIR, \"pooled_keywords_top5_long.csv\")\n",
    "kw_long.to_csv(kw_long_path, index=False, encoding=\"utf-8-sig\")\n",
    "print(\"[save]\", kw_long_path)\n",
    "\n",
    "# ===============================\n",
    "# 2) 키워드 자체를 벡터화\n",
    "#    - 키워드가 짧으니 char n-gram이 안정적\n",
    "#    - 공백이 있는 키워드(예: 'injury prevention')도 잘 커버\n",
    "# ===============================\n",
    "vectorizer_kw = TfidfVectorizer(\n",
    "    analyzer=\"char_wb\",       # 단어 경계 기반 문자 n-gram\n",
    "    ngram_range=(3,5),        # 3~5그램 권장(짧은 토큰에 강함)\n",
    "    min_df=1\n",
    ")\n",
    "X_kw = vectorizer_kw.fit_transform(kw_long[\"keyword\"])\n",
    "print(\"X_kw shape:\", X_kw.shape)\n",
    "\n",
    "# ===============================\n",
    "# 3) KMeans 클러스터링 (k는 필요에 따라 조절)\n",
    "#    - 실루엣으로 후보 k 점검 후 최종 k 선택\n",
    "# ===============================\n",
    "k_candidates = [4,5,6,7,8]\n",
    "scores = []\n",
    "for k in k_candidates:\n",
    "    if X_kw.shape[0] > k:\n",
    "        km_tmp = KMeans(n_clusters=k, n_init=\"auto\", random_state=42)\n",
    "        labels_tmp = km_tmp.fit_predict(X_kw)\n",
    "        scores.append(silhouette_score(X_kw, labels_tmp))\n",
    "    else:\n",
    "        scores.append(np.nan)\n",
    "\n",
    "sil_df = pd.DataFrame({\"k\": k_candidates, \"silhouette\": scores})\n",
    "print(sil_df)\n",
    "\n",
    "# 최종 k 선택(직접 지정해도 됨)\n",
    "K_CLUSTERS = int(sil_df.iloc[sil_df[\"silhouette\"].idxmax()][\"k\"]) if sil_df[\"silhouette\"].notna().any() else 6\n",
    "print(\"[info] choose k =\", K_CLUSTERS)\n",
    "\n",
    "km = KMeans(n_clusters=K_CLUSTERS, n_init=\"auto\", random_state=42)\n",
    "kw_long[\"cluster\"] = km.fit_predict(X_kw)\n",
    "\n",
    "clusters_csv = os.path.join(OUTDIR, f\"{YEAR_TARGET}keyword_clusters_k{K_CLUSTERS}.csv\")\n",
    "kw_long.to_csv(clusters_csv, index=False, encoding=\"utf-8-sig\")\n",
    "print(\"[save]\", clusters_csv)\n",
    "\n",
    "# ===============================\n",
    "# 4) 클러스터 요약 (빈도/대표 키워드/대표 문서)\n",
    "# ===============================\n",
    "# (a) 클러스터별 키워드 빈도 상위 샘플\n",
    "top_kw_by_cluster = (\n",
    "    kw_long.groupby([\"cluster\",\"keyword\"])\n",
    "           .size()\n",
    "           .reset_index(name=\"freq\")\n",
    "           .sort_values([\"cluster\",\"freq\"], ascending=[True, False])\n",
    ")\n",
    "\n",
    "summary_rows = []\n",
    "for c in sorted(kw_long[\"cluster\"].unique()):\n",
    "    sample = top_kw_by_cluster[top_kw_by_cluster[\"cluster\"]==c].head(15)[\"keyword\"].tolist()\n",
    "    summary_rows.append({\"cluster\": c, \"sample_keywords\": \", \".join(sample)})\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "summary_csv = os.path.join(OUTDIR, f\"{YEAR_TARGET}keyword_cluster_summary_k{K_CLUSTERS}.csv\")\n",
    "summary_df.to_csv(summary_csv, index=False, encoding=\"utf-8-sig\")\n",
    "print(\"[save]\", summary_csv)\n",
    "\n",
    "# (b) 클러스터별 대표 문서(해당 클러스터에 속한 키워드의 tf-idf 합 기준 상위)\n",
    "doc_rank = (\n",
    "    kw_long.groupby([\"cluster\",\"title\"])[\"tfidf\"]\n",
    "           .sum()\n",
    "           .reset_index()\n",
    "           .sort_values([\"cluster\",\"tfidf\"], ascending=[True, False])\n",
    ")\n",
    "top_docs = doc_rank.groupby(\"cluster\").head(10)\n",
    "top_docs_csv = os.path.join(OUTDIR, f\"{YEAR_TARGET}keyword_cluster_top_docs_k{K_CLUSTERS}.csv\")\n",
    "top_docs.to_csv(top_docs_csv, index=False, encoding=\"utf-8-sig\")\n",
    "print(\"[save]\", top_docs_csv)\n",
    "\n",
    "# ===============================\n",
    "# 5) 화면에 간단 프린트\n",
    "# ===============================\n",
    "for c in sorted(kw_long[\"cluster\"].unique()):\n",
    "    sample_kw = top_kw_by_cluster[top_kw_by_cluster[\"cluster\"]==c].head(10)[\"keyword\"].tolist()\n",
    "    print(f\"\\n=== Cluster {c} ===\")\n",
    "    print(\", \".join(sample_kw))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
