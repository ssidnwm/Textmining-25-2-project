{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T01:06:11.939772Z",
     "iopub.status.busy": "2025-10-04T01:06:11.939772Z",
     "iopub.status.idle": "2025-10-04T01:06:11.951492Z",
     "shell.execute_reply": "2025-10-04T01:06:11.950858Z"
    },
    "tags": [
     "injected_params"
    ]
   },
   "outputs": [],
   "source": [
    "INPUT_CSV = r\"C:\\Users\\82109\\Desktop\\벼리\\7학기\\텍스트마이닝\\scripts\\텍스트마이닝_초록_검사결과_전체컬럼1618.csv\"\n",
    "INPUT_XLSX = INPUT_CSV\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T01:06:11.953541Z",
     "iopub.status.busy": "2025-10-04T01:06:11.953541Z",
     "iopub.status.idle": "2025-10-04T01:06:11.966973Z",
     "shell.execute_reply": "2025-10-04T01:06:11.966413Z"
    },
    "tags": [
     "injected_params"
    ]
   },
   "outputs": [],
   "source": [
    "OUTDIR = r\"C:\\Users\\82109\\Desktop\\벼리\\7학기\\텍스트마이닝\\방법론2\\TF-IDF1618\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T01:06:11.969541Z",
     "iopub.status.busy": "2025-10-04T01:06:11.969030Z",
     "iopub.status.idle": "2025-10-04T01:06:12.881889Z",
     "shell.execute_reply": "2025-10-04T01:06:12.881889Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T01:06:12.884890Z",
     "iopub.status.busy": "2025-10-04T01:06:12.883890Z",
     "iopub.status.idle": "2025-10-04T01:06:12.896890Z",
     "shell.execute_reply": "2025-10-04T01:06:12.896890Z"
    }
   },
   "outputs": [],
   "source": [
    "# 분석할 '그 파일'의 정확한 경로를 직접 넣으세요\n",
    "INPUT_CSV = r\"C:\\Users\\82109\\Desktop\\벼리\\7학기\\텍스트마이닝\\scripts\\텍스트마이닝_초록_검사결과_전체컬럼1618.csv\"\n",
    "TITLE_COL  = \"제목\"     # 없으면 None\n",
    "ABS_COL = \"abstract\"\n",
    "\n",
    "# ===== TF-IDF 옵션 =====\n",
    "USE_BIGRAMS = True   # (1,2) n-gram 사용할지\n",
    "MIN_DF      = 2      # 너무 희귀한 토큰 제거(문서 수 기준)\n",
    "MAX_DF      = 0.95   # 너무 흔한 토큰 제거(문서 비율 기준)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T01:06:12.898901Z",
     "iopub.status.busy": "2025-10-04T01:06:12.898901Z",
     "iopub.status.idle": "2025-10-04T01:06:12.912902Z",
     "shell.execute_reply": "2025-10-04T01:06:12.912497Z"
    }
   },
   "outputs": [],
   "source": [
    "# 한글/영문/숫자 토큰 추출용 정규식 (길이 >= 2)\n",
    "# 예: \"부상 예방\", \"soccer\", \"2019\" → 토큰\n",
    "_TOKEN_RE = re.compile(r\"[가-힣]{2,}|[A-Za-z]{2,}|[0-9]{2,}\")\n",
    "\n",
    "# 필요시 도메인 불용어(자유롭게 추가/수정)\n",
    "STOPWORDS = {\n",
    "    \"연구\", \"결과\", \"방법\", \"분석\", \"본\", \"최근\", \"활용\", \"제시\", \"문헌\",\n",
    "    \"the\", \"and\", \"for\", \"with\", \"from\", \"using\", \"based\", \"study\", \"results\"\n",
    "}\n",
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "    if not isinstance(s, str): return \"\"\n",
    "    s = s.lower()\n",
    "    # 도메인 규칙: fifa 11+ → fifa11plus 로 묶어주기\n",
    "    s = re.sub(r\"fifa\\s*11\\+?\", \"fifa11plus\", s, flags=re.I)\n",
    "    # URL/특수기호 정리\n",
    "    s = re.sub(r\"https?://\\S+|www\\.\\S+\", \" \", s)\n",
    "    s = re.sub(r\"[\\u200b-\\u200d\\ufeff]\", \" \", s)\n",
    "    s = re.sub(r\"[^\\w\\s가-힣+]\", \" \", s)   # +는 11+ 같은 표기를 위해 남겨둠\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "# “문자(한글/영문)가 최소 1개 포함된” 토큰만 추출\n",
    "_TOKEN_RE = re.compile(r\"(?=[\\w+]*[가-힣A-Za-z])[가-힣A-Za-z0-9+]{2,}\")\n",
    "\n",
    "\n",
    "def regex_tokenize(text: str):\n",
    "    text = normalize_text(text)\n",
    "    toks = _TOKEN_RE.findall(text)\n",
    "    # 숫자만으로 구성된 토큰 제거(이중 안전장치)\n",
    "    toks = [t for t in toks if not t.isdigit()]\n",
    "    # 불용어 제거\n",
    "    toks = [t for t in toks if t not in STOPWORDS]\n",
    "    return toks\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T01:06:12.914906Z",
     "iopub.status.busy": "2025-10-04T01:06:12.914906Z",
     "iopub.status.idle": "2025-10-04T01:06:13.258049Z",
     "shell.execute_reply": "2025-10-04T01:06:13.258049Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF shape: (765, 20019)\n",
      "예시 terms: ['10g' '1rm' '1rm was' '1st' '20g' '2c' '2d' '2m' '2n' '2nd' '3d'\n",
      " '3d kinematic' '3d kinematics' '3d motion' '3d video' '3rd' '3x'\n",
      " '3x week' '40g' '50th']\n"
     ]
    }
   ],
   "source": [
    "# 1) 데이터 로드\n",
    "assert os.path.exists(INPUT_CSV), f\"파일이 없습니다: {INPUT_XLSX}\"\n",
    "df = pd.read_csv(INPUT_CSV)\n",
    "\n",
    "assert ABS_COL in df.columns, f\"'{ABS_COL}' 컬럼을 찾을 수 없습니다.\"\n",
    "texts = df[ABS_COL].astype(str).tolist()\n",
    "\n",
    "# 2) TF-IDF 빌드 (정규식 토크나이저 사용)\n",
    "ngram = (1, 2) if USE_BIGRAMS else (1, 1)\n",
    "vectorizer = TfidfVectorizer(\n",
    "    tokenizer=regex_tokenize,   # 커스텀 토크나이저\n",
    "    token_pattern=None,         # <- 커스텀 tokenizer 쓸 때 반드시 None\n",
    "    ngram_range=ngram,\n",
    "    min_df=MIN_DF,\n",
    "    max_df=MAX_DF,\n",
    "    stop_words=list(STOPWORDS)\n",
    ")\n",
    "X = vectorizer.fit_transform(texts)               # (문서수, 용어수) CSR 희소행렬\n",
    "terms = np.array(vectorizer.get_feature_names_out())\n",
    "\n",
    "print(\"TF-IDF shape:\", X.shape)\n",
    "print(\"예시 terms:\", terms[:20])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "상위 키워드 5개씩 뽑기 - 논문당"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T01:06:13.267050Z",
     "iopub.status.busy": "2025-10-04T01:06:13.267050Z",
     "iopub.status.idle": "2025-10-04T01:06:13.321936Z",
     "shell.execute_reply": "2025-10-04T01:06:13.321429Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[save] C:\\Users\\82109\\Desktop\\벼리\\7학기\\텍스트마이닝\\방법론2\\TF-IDF1618\\tfidf_top5_per_doc.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>top5_keywords</th>\n",
       "      <th>kw1</th>\n",
       "      <th>kw2</th>\n",
       "      <th>kw3</th>\n",
       "      <th>kw4</th>\n",
       "      <th>kw5</th>\n",
       "      <th>score1</th>\n",
       "      <th>score2</th>\n",
       "      <th>score3</th>\n",
       "      <th>score4</th>\n",
       "      <th>score5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The VEGFA gene and anterior cruciate ligament ...</td>\n",
       "      <td>2018-12-28</td>\n",
       "      <td>2018</td>\n",
       "      <td>aclr(0.463), aclr in(0.171), allele(0.164), gr...</td>\n",
       "      <td>aclr</td>\n",
       "      <td>aclr in</td>\n",
       "      <td>allele</td>\n",
       "      <td>group consisted</td>\n",
       "      <td>group</td>\n",
       "      <td>0.463</td>\n",
       "      <td>0.171</td>\n",
       "      <td>0.164</td>\n",
       "      <td>0.164</td>\n",
       "      <td>0.162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The relationship between ACTN3 R577X gene poly...</td>\n",
       "      <td>2018-12-28</td>\n",
       "      <td>2018</td>\n",
       "      <td>genotyped(0.308), distribution(0.283), polymor...</td>\n",
       "      <td>genotyped</td>\n",
       "      <td>distribution</td>\n",
       "      <td>polymorphism</td>\n",
       "      <td>arm swing</td>\n",
       "      <td>arm</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.283</td>\n",
       "      <td>0.213</td>\n",
       "      <td>0.207</td>\n",
       "      <td>0.177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Reliability and validity of field‐based fitnes...</td>\n",
       "      <td>2018-12-27</td>\n",
       "      <td>2018</td>\n",
       "      <td>fitness tests(0.254), tests(0.236), fitness(0....</td>\n",
       "      <td>fitness tests</td>\n",
       "      <td>tests</td>\n",
       "      <td>fitness</td>\n",
       "      <td>standards</td>\n",
       "      <td>field</td>\n",
       "      <td>0.254</td>\n",
       "      <td>0.236</td>\n",
       "      <td>0.215</td>\n",
       "      <td>0.183</td>\n",
       "      <td>0.172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A mathematical model for decision-making in th...</td>\n",
       "      <td>2018-12-24</td>\n",
       "      <td>2018</td>\n",
       "      <td>classification(0.306), impairment(0.208), pals...</td>\n",
       "      <td>classification</td>\n",
       "      <td>impairment</td>\n",
       "      <td>palsy</td>\n",
       "      <td>cerebral palsy</td>\n",
       "      <td>para</td>\n",
       "      <td>0.306</td>\n",
       "      <td>0.208</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.176</td>\n",
       "      <td>0.168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Postural Control Deficits After Repetitive Soc...</td>\n",
       "      <td>2018-12-15</td>\n",
       "      <td>2018</td>\n",
       "      <td>heading(0.245), soccer heading(0.232), postura...</td>\n",
       "      <td>heading</td>\n",
       "      <td>soccer heading</td>\n",
       "      <td>postural control</td>\n",
       "      <td>control</td>\n",
       "      <td>postural</td>\n",
       "      <td>0.245</td>\n",
       "      <td>0.232</td>\n",
       "      <td>0.226</td>\n",
       "      <td>0.211</td>\n",
       "      <td>0.208</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title        date  year  \\\n",
       "0  The VEGFA gene and anterior cruciate ligament ...  2018-12-28  2018   \n",
       "1  The relationship between ACTN3 R577X gene poly...  2018-12-28  2018   \n",
       "2  Reliability and validity of field‐based fitnes...  2018-12-27  2018   \n",
       "3  A mathematical model for decision-making in th...  2018-12-24  2018   \n",
       "4  Postural Control Deficits After Repetitive Soc...  2018-12-15  2018   \n",
       "\n",
       "                                       top5_keywords             kw1  \\\n",
       "0  aclr(0.463), aclr in(0.171), allele(0.164), gr...            aclr   \n",
       "1  genotyped(0.308), distribution(0.283), polymor...       genotyped   \n",
       "2  fitness tests(0.254), tests(0.236), fitness(0....   fitness tests   \n",
       "3  classification(0.306), impairment(0.208), pals...  classification   \n",
       "4  heading(0.245), soccer heading(0.232), postura...         heading   \n",
       "\n",
       "              kw2               kw3              kw4       kw5 score1 score2  \\\n",
       "0         aclr in            allele  group consisted     group  0.463  0.171   \n",
       "1    distribution      polymorphism        arm swing       arm  0.308  0.283   \n",
       "2           tests           fitness        standards     field  0.254  0.236   \n",
       "3      impairment             palsy   cerebral palsy      para  0.306  0.208   \n",
       "4  soccer heading  postural control          control  postural  0.245  0.232   \n",
       "\n",
       "  score3 score4 score5  \n",
       "0  0.164  0.164  0.162  \n",
       "1  0.213  0.207  0.177  \n",
       "2  0.215  0.183  0.172  \n",
       "3  0.176  0.176  0.168  \n",
       "4  0.226  0.211  0.208  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "TOPK_PER_DOC = 5  # 문서당 추출할 키워드 개수\n",
    "\n",
    "def topk_keywords_per_doc(X, terms, k=5):\n",
    "    \"\"\"각 문서의 TF-IDF 상위 k개 (term, score) 리스트 반환\"\"\"\n",
    "    results = []\n",
    "    for i in range(X.shape[0]):\n",
    "        row = X.getrow(i)\n",
    "        if row.nnz == 0:\n",
    "            results.append([])\n",
    "            continue\n",
    "        # 상위 k개 인덱스 (k보다 작으면 가능한 만큼)\n",
    "        take = min(k, row.nnz)\n",
    "        part = np.argpartition(row.data, -take)[-take:]      # 상위 k개 *무정렬*\n",
    "        order = part[np.argsort(row.data[part])[::-1]]       # TF-IDF 내림차순 정렬\n",
    "        term_idx = row.indices[order]\n",
    "        scores = row.data[order]\n",
    "        results.append(list(zip(terms[term_idx], scores)))\n",
    "    return results\n",
    "\n",
    "# 1) 문서별 상위 5 키워드 (term, score) 추출\n",
    "topk = topk_keywords_per_doc(X, terms, k=TOPK_PER_DOC)\n",
    "\n",
    "# 2) 결과 DataFrame 구성: 제목 + 상위키워드(문자열)\n",
    "title = df['title'] if \"title\" in df.columns else pd.Series([\"\"] * len(df))\n",
    "date = df[\"date\"] if \"date\" in df.columns else pd.Series([\"\"] * len(df))\n",
    "year = df[\"year\"] if \"year\" in df.columns else pd.Series([\"\"] * len(df))\n",
    "topk_str = [\", \".join([f\"{t}({s:.3f})\" for t, s in pairs]) for pairs in topk]\n",
    "\n",
    "# (옵션) 개별 컬럼으로도 뽑기: kw1..kw5, score1..score5\n",
    "def pad(lst, n, fill=None):  # 길이가 n보다 짧으면 채워 넣기\n",
    "    return lst + [fill] * (n - len(lst))\n",
    "\n",
    "kw_cols = []\n",
    "sc_cols = []\n",
    "for pairs in topk:\n",
    "    terms_only  = pad([t for t, _ in pairs], TOPK_PER_DOC, \"\")\n",
    "    scores_only = pad([f\"{s:.3f}\" for _, s in pairs], TOPK_PER_DOC, \"\")\n",
    "    kw_cols.append(terms_only)\n",
    "    sc_cols.append(scores_only)\n",
    "\n",
    "kw_df = pd.DataFrame(kw_cols, columns=[f\"kw{i+1}\" for i in range(TOPK_PER_DOC)])\n",
    "sc_df = pd.DataFrame(sc_cols, columns=[f\"score{i+1}\" for i in range(TOPK_PER_DOC)])\n",
    "\n",
    "result_df = pd.DataFrame({\"title\": title, \"date\": date,\"year\": year,\"top5_keywords\": topk_str})\n",
    "result_df = pd.concat([result_df, kw_df, sc_df], axis=1)\n",
    "\n",
    "# 3) 저장\n",
    "out_path = os.path.join(OUTDIR, f\"tfidf_top{TOPK_PER_DOC}_per_doc.csv\")\n",
    "result_df.to_csv(out_path, index=False, encoding=\"utf-8-sig\")\n",
    "print(\"[save]\", out_path)\n",
    "\n",
    "# 4) 샘플 확인 (앞 5개)\n",
    "result_df.head()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제, 이 논문들의 키워드를 가지고 클러스터링을 진행함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-04T01:06:13.323941Z",
     "iopub.status.busy": "2025-10-04T01:06:13.323941Z",
     "iopub.status.idle": "2025-10-04T01:06:14.682851Z",
     "shell.execute_reply": "2025-10-04T01:06:14.681851Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] pooled keywords: (1420, 5) unique: 1121\n",
      "[save] ./TFIDF2018\\pooled_keywords_top5_long.csv\n",
      "X_kw shape: (1420, 7922)\n",
      "   k  silhouette\n",
      "0  4    0.006440\n",
      "1  5    0.011522\n",
      "2  6    0.012511\n",
      "3  7    0.013660\n",
      "4  8    0.011868\n",
      "[info] choose k = 7\n",
      "[save] ./TFIDF2018\\2018keyword_clusters_k7.csv\n",
      "[save] ./TFIDF2018\\2018keyword_cluster_summary_k7.csv\n",
      "[save] ./TFIDF2018\\2018keyword_cluster_top_docs_k7.csv\n",
      "\n",
      "=== Cluster 0 ===\n",
      "gps, lps, greater quadriceps, groups, jumps, mps, ps, quadriceps, quadriceps hamstrings, to quadriceps\n",
      "\n",
      "=== Cluster 1 ===\n",
      "pain, game, groin pain, intervention, programme, asymmetry, athlete, back pain, filter, an outer\n",
      "\n",
      "=== Cluster 2 ===\n",
      "classification, fitness, test, cutting, estimation, mechanical, mechanism, neck, physical fitness, rates\n",
      "\n",
      "=== Cluster 3 ===\n",
      "acl, concussion, ball, force, h3, head, hip, cod, exposure, strength\n",
      "\n",
      "=== Cluster 4 ===\n",
      "injuries, extremity injury, groin injuries, injury, of injury, sports injuries, acl injured, acl injuries, acl injury, ankle injuries\n",
      "\n",
      "=== Cluster 5 ===\n",
      "running, kicking, brain, hamstring, landing, training, walking, heading, jumping, learning\n",
      "\n",
      "=== Cluster 6 ===\n",
      "season, postural, directional, dual task, experimental, experimental group, inertial, intensity, isokinetic, motor\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "assert \"year\" in df.columns, \" df에 'year' 컬럼이 있어야 합니다.\"\n",
    "assert \"title\" in df.columns, \" df에 'title' 컬럼이 있어야 합니다.\"\n",
    "\n",
    "YEAR_TARGET = 2018         \n",
    "TITLE_COL   = \"title\"\n",
    "DATE_COL    = \"date\"\n",
    "YEAR_COL    = \"year\"\n",
    "\n",
    "OUTDIR = \"./TFIDF2018\" # 여기 연도별로 바꾸기\n",
    "os.makedirs(OUTDIR, exist_ok=True)\n",
    "\n",
    "mask_year = (df[YEAR_COL] == YEAR_TARGET)\n",
    "df_year = df.loc[mask_year].reset_index(drop=True)\n",
    "if df_year.empty:\n",
    "    raise ValueError(f\"{YEAR_TARGET}년에 해당하는 문서가 없습니다.\")\n",
    "\n",
    "idx_year = np.where(mask_year.values)[0]\n",
    "topk_year = [topk[i] for i in idx_year]\n",
    "# ===============================\n",
    "# 1) 문서별 top-k 결과(topk)를 \"길게\" 풀기\n",
    "#    topk: List[List[(term, score)]]\n",
    "# ===============================\n",
    "rows = []\n",
    "for i, pairs in enumerate(topk_year):\n",
    "    title = df[TITLE_COL].iloc[i] if TITLE_COL in df.columns else i\n",
    "    for term, val in pairs:\n",
    "        rows.append({\n",
    "            \"year\": YEAR_TARGET,\n",
    "            \"doc_id\": i,\n",
    "            \"title\": title,\n",
    "            \"keyword\": term,\n",
    "            \"tfidf\": float(val)\n",
    "        })\n",
    "\n",
    "kw_long = pd.DataFrame(rows)\n",
    "print(\"[info] pooled keywords:\", kw_long.shape, \"unique:\", kw_long[\"keyword\"].nunique())\n",
    "kw_long_path = os.path.join(OUTDIR, \"pooled_keywords_top5_long.csv\")\n",
    "kw_long.to_csv(kw_long_path, index=False, encoding=\"utf-8-sig\")\n",
    "print(\"[save]\", kw_long_path)\n",
    "\n",
    "# ===============================\n",
    "# 2) 키워드 자체를 벡터화\n",
    "#    - 키워드가 짧으니 char n-gram이 안정적\n",
    "#    - 공백이 있는 키워드(예: 'injury prevention')도 잘 커버\n",
    "# ===============================\n",
    "vectorizer_kw = TfidfVectorizer(\n",
    "    analyzer=\"char_wb\",       # 단어 경계 기반 문자 n-gram\n",
    "    ngram_range=(3,5),        # 3~5그램 권장(짧은 토큰에 강함)\n",
    "    min_df=1\n",
    ")\n",
    "X_kw = vectorizer_kw.fit_transform(kw_long[\"keyword\"])\n",
    "print(\"X_kw shape:\", X_kw.shape)\n",
    "\n",
    "# ===============================\n",
    "# 3) KMeans 클러스터링 (k는 필요에 따라 조절)\n",
    "#    - 실루엣으로 후보 k 점검 후 최종 k 선택\n",
    "# ===============================\n",
    "k_candidates = [4,5,6,7,8]\n",
    "scores = []\n",
    "for k in k_candidates:\n",
    "    if X_kw.shape[0] > k:\n",
    "        km_tmp = KMeans(n_clusters=k, n_init=\"auto\", random_state=42)\n",
    "        labels_tmp = km_tmp.fit_predict(X_kw)\n",
    "        scores.append(silhouette_score(X_kw, labels_tmp))\n",
    "    else:\n",
    "        scores.append(np.nan)\n",
    "\n",
    "sil_df = pd.DataFrame({\"k\": k_candidates, \"silhouette\": scores})\n",
    "print(sil_df)\n",
    "\n",
    "# 최종 k 선택(직접 지정해도 됨)\n",
    "K_CLUSTERS = int(sil_df.iloc[sil_df[\"silhouette\"].idxmax()][\"k\"]) if sil_df[\"silhouette\"].notna().any() else 6\n",
    "print(\"[info] choose k =\", K_CLUSTERS)\n",
    "\n",
    "km = KMeans(n_clusters=K_CLUSTERS, n_init=\"auto\", random_state=42)\n",
    "kw_long[\"cluster\"] = km.fit_predict(X_kw)\n",
    "\n",
    "clusters_csv = os.path.join(OUTDIR, f\"{YEAR_TARGET}keyword_clusters_k{K_CLUSTERS}.csv\")\n",
    "kw_long.to_csv(clusters_csv, index=False, encoding=\"utf-8-sig\")\n",
    "print(\"[save]\", clusters_csv)\n",
    "\n",
    "# ===============================\n",
    "# 4) 클러스터 요약 (빈도/대표 키워드/대표 문서)\n",
    "# ===============================\n",
    "# (a) 클러스터별 키워드 빈도 상위 샘플\n",
    "top_kw_by_cluster = (\n",
    "    kw_long.groupby([\"cluster\",\"keyword\"])\n",
    "           .size()\n",
    "           .reset_index(name=\"freq\")\n",
    "           .sort_values([\"cluster\",\"freq\"], ascending=[True, False])\n",
    ")\n",
    "\n",
    "summary_rows = []\n",
    "for c in sorted(kw_long[\"cluster\"].unique()):\n",
    "    sample = top_kw_by_cluster[top_kw_by_cluster[\"cluster\"]==c].head(15)[\"keyword\"].tolist()\n",
    "    summary_rows.append({\"cluster\": c, \"sample_keywords\": \", \".join(sample)})\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "summary_csv = os.path.join(OUTDIR, f\"{YEAR_TARGET}keyword_cluster_summary_k{K_CLUSTERS}.csv\")\n",
    "summary_df.to_csv(summary_csv, index=False, encoding=\"utf-8-sig\")\n",
    "print(\"[save]\", summary_csv)\n",
    "\n",
    "# (b) 클러스터별 대표 문서(해당 클러스터에 속한 키워드의 tf-idf 합 기준 상위)\n",
    "doc_rank = (\n",
    "    kw_long.groupby([\"cluster\",\"title\"])[\"tfidf\"]\n",
    "           .sum()\n",
    "           .reset_index()\n",
    "           .sort_values([\"cluster\",\"tfidf\"], ascending=[True, False])\n",
    ")\n",
    "top_docs = doc_rank.groupby(\"cluster\").head(10)\n",
    "top_docs_csv = os.path.join(OUTDIR, f\"{YEAR_TARGET}keyword_cluster_top_docs_k{K_CLUSTERS}.csv\")\n",
    "top_docs.to_csv(top_docs_csv, index=False, encoding=\"utf-8-sig\")\n",
    "print(\"[save]\", top_docs_csv)\n",
    "\n",
    "# ===============================\n",
    "# 5) 화면에 간단 프린트\n",
    "# ===============================\n",
    "for c in sorted(kw_long[\"cluster\"].unique()):\n",
    "    sample_kw = top_kw_by_cluster[top_kw_by_cluster[\"cluster\"]==c].head(10)[\"keyword\"].tolist()\n",
    "    print(f\"\\n=== Cluster {c} ===\")\n",
    "    print(\", \".join(sample_kw))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
